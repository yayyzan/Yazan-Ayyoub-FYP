{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ffdd2c19",
      "metadata": {
        "id": "ffdd2c19"
      },
      "source": [
        "# ðŸ“˜ Multimodal Option Price Prediction Workbook\n",
        "\n",
        "This notebook implements the full pipeline for multimodal option price prediction, including data ingestion, feature engineering, model training (with hyperparameter search), and evaluation. It follows a modular, professional, and academically-aligned coding structure and served as the main development and experimentation environment for this project.\n",
        "\n",
        "---\n",
        "\n",
        "### âš ï¸ **Note on Data Access**\n",
        "\n",
        "This notebook references input files stored in a private Google Drive directory. These files include licensed datasets (from **OptionMetrics IvyDB USA**) that **cannot be redistributed or publicly shared** due to legal and licensing restrictions clarified by the library. As such, this notebook will not execute end-to-end without access to those files.  To comply with licensing terms, the data has been kept private and is **not included in this repository**. Only derived, non-reversible processed data (such as `inference_eval.csv`) is made available for safe model evaluation.\n",
        "\n",
        "However, the code is still included for **transparency and inspection**.\n",
        "\n",
        "### **Key Design Principles**\n",
        "\n",
        "1. **Centralized configuration**: All hyperparameters and constants are defined in a `Config` class.\n",
        "2. **Modular functions**: Data processing and modeling logic are encapsulated in reusable, testable functions.\n",
        "3. **Unified logging**: Consistent use of logging throughout, with optional progress bars.\n",
        "4. **Structured layout**: Section headers follow the IMRaD convention (Introduction â†’ Methods â†’ Results â†’ Discussion) for clarity and organization.\n",
        "\n",
        "> ðŸ’¡ **To reproduce final results**, please run the streamlined evaluation notebook: `Inference_Demo.ipynb`.  \n",
        "> This notebook is intended for experimentation and **not optimized for CPU-only environments**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "707486f7",
      "metadata": {
        "id": "707486f7"
      },
      "source": [
        "## 0Â Â Environment &Â Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "831c6ebf",
      "metadata": {
        "id": "831c6ebf"
      },
      "outputs": [],
      "source": [
        "!pip install -qU fredapi yfinance pytrends pandas requests beautifulsoup4 matplotlib seaborn transformers sentencepiece tqdm optuna ray[tune] torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Google Drive mount (Colab) ===========================================\n",
        "# Run this once per session; it will prompt you to grant Colab access.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)   # force_remount avoids \"already mounted\" errors\n",
        "\n",
        "# (optional) quick sanity-check that the data files are visible\n",
        "!ls -lh /content/drive/MyDrive | grep -E 'spx_price|vix_price|spx_options' || echo \"Files not found â€“ double-check paths\"\n"
      ],
      "metadata": {
        "id": "RrAZ1IWv_2pl"
      },
      "id": "RrAZ1IWv_2pl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ee9be522",
      "metadata": {
        "id": "ee9be522"
      },
      "source": [
        "## 1Â Â ImportsÂ &Â Global Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3440f36a",
      "metadata": {
        "id": "3440f36a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import logging\n",
        "import time\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Tuple\n",
        "import joblib\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "import yfinance as yf\n",
        "from fredapi import Fred\n",
        "\n",
        "import optuna\n",
        "from tqdm.auto import tqdm\n",
        "from ray import tune, train\n",
        "\n",
        "# ---------- Configuration ----------\n",
        "@dataclass\n",
        "class Config:\n",
        "    # Use cache? (for faster running time, set False for Black Scholes check)\n",
        "    USE_CACHED = True\n",
        "\n",
        "    # Paths\n",
        "    DRIVE_ROOT: Path = Path('/content/drive/MyDrive')\n",
        "    FIN_NEWS_PATH: Path = DRIVE_ROOT / 'FinSen_US_Categorized_Timestamp.csv'\n",
        "    OPTIONS_CSV_PATH: Path = DRIVE_ROOT / 'spx_options.csv'\n",
        "    SPX_CSV_PATH: Path = DRIVE_ROOT / 'spx_price.csv'\n",
        "    VIX_CSV_PATH: Path = DRIVE_ROOT / 'vix_price.csv'\n",
        "    OUTPUT_DIR: Path = Path('/content/option_price_plots')\n",
        "    SAVED_MODEL_DIR: Path ='trained_models'\n",
        "\n",
        "    # FRED\n",
        "    FRED_API_KEY: str = 'b73d6a590f43c3d04bde3404b960c821'\n",
        "\n",
        "    # Timeâ€‘window\n",
        "    START_DATE: str = '2018-01-01'\n",
        "    END_DATE:   str = '2023-12-31'\n",
        "    WINDOW: int = 10\n",
        "\n",
        "    # Hardware\n",
        "    DEVICE: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Features\n",
        "    MACRO_COLS:  List[str] = field(default_factory=lambda: [\n",
        "        '10y_treasury'\n",
        "    ])\n",
        "    SENTIMENT_COLS: List[str] = field(default_factory=lambda: ['sentiment_score', 'market_sentiment'])\n",
        "    OPTION_COLS: List[str] = field(default_factory=lambda: [\n",
        "        'vix_price', 'spx_price', 'impl_volatility',\n",
        "        'moneyness', 'open_interest', 'greeks_signal_1'\n",
        "    ])\n",
        "\n",
        "CFG = Config()\n",
        "os.makedirs(CFG.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# ---------- Logging ----------\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='[%(asctime)s] %(levelname)s â€” %(message)s',\n",
        "                    datefmt='%H:%M:%S')\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.info(f'Using deviceÂ â†’ {CFG.DEVICE}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5f7c19d",
      "metadata": {
        "id": "e5f7c19d"
      },
      "source": [
        "## 2Â Â UtilityÂ Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "512355b3",
      "metadata": {
        "id": "512355b3"
      },
      "outputs": [],
      "source": [
        "def scale_series(series: pd.DataFrame) -> np.ndarray:\n",
        "    \"\"\"Minâ€‘max scale a single or multiâ€‘column DataFrame and return NumPy array.\"\"\"\n",
        "    return MinMaxScaler().fit_transform(series)\n",
        "\n",
        "def forward_pass(model: nn.Module, batch):\n",
        "    \"\"\"Helper to accommodate the three dataset return formats.\"\"\"\n",
        "    if len(batch) == 2:                           # FNN / earlyâ€‘fusion flat\n",
        "        x, y = (t.to(CFG.DEVICE) for t in batch)\n",
        "        preds = model(x).squeeze(-1)\n",
        "    elif len(batch) == 4:                         # Multimodal\n",
        "        m, s, o, y = (t.to(CFG.DEVICE) for t in batch)\n",
        "        preds = model(m, s, o).squeeze(-1)\n",
        "    else:\n",
        "        raise ValueError(f'Unexpected batch sizeÂ {len(batch)}')\n",
        "    return preds, y.to(CFG.DEVICE)\n",
        "\n",
        "def split_dataset(df, window, train_frac=0.7, val_frac=0.15):\n",
        "    total = len(df) - window\n",
        "    t_idx = int(train_frac * total)\n",
        "    v_idx = int((train_frac + val_frac) * total)\n",
        "\n",
        "    train_df = df.iloc[:t_idx]\n",
        "    val_df   = df.iloc[t_idx - window:v_idx]\n",
        "    test_df  = df.iloc[v_idx - window:]\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "def scale_train_test(train_df, test_df, cols):\n",
        "    scaler = StandardScaler()\n",
        "    train_scaled = train_df.copy()\n",
        "    test_scaled = test_df.copy()\n",
        "    train_scaled[cols] = scaler.fit_transform(train_df[cols])\n",
        "    test_scaled[cols] = scaler.transform(test_df[cols])\n",
        "    return train_scaled, test_scaled, scaler  # <- add scaler as third return\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "173999df",
      "metadata": {
        "id": "173999df"
      },
      "source": [
        "## 3Â Â Data AcquisitionÂ &Â Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e4c3a36",
      "metadata": {
        "id": "4e4c3a36"
      },
      "outputs": [],
      "source": [
        "# --- 3.1Â Macroâ€‘economic timeâ€‘series ---\n",
        "def fetch_fast_macro_data(start: str, end: str) -> pd.DataFrame:\n",
        "    fred = Fred(api_key=CFG.FRED_API_KEY)\n",
        "    data = {\n",
        "        '10y_treasury': fred.get_series('DGS10', start, end),\n",
        "        '2y_treasury':  fred.get_series('DGS2',  start, end),\n",
        "        'high_yield_spread': fred.get_series('BAMLH0A0HYM2EY', start, end)\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    df['yield_curve_slope'] = df['10y_treasury'] - df['2y_treasury']\n",
        "    return df.resample('D').ffill()\n",
        "\n",
        "# --- 3.3Â FinBERT pipeline (news sentiment) ---\n",
        "tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')\n",
        "finbert   = AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert').to(CFG.DEVICE)\n",
        "sentiment_pipeline = pipeline('sentiment-analysis', model=finbert, tokenizer=tokenizer,\n",
        "                              device=0 if CFG.DEVICE=='cuda' else -1, batch_size=32)\n",
        "\n",
        "def fetch_news_sentiment(csv_path: Path, start: str, end: str) -> pd.DataFrame:\n",
        "    raw = pd.read_csv(csv_path, parse_dates=['Time'], dayfirst=True)\n",
        "    raw.columns = [c.strip().lower() for c in raw.columns]\n",
        "    raw = raw[(raw['time'] >= start) & (raw['time'] <= end)].dropna(subset=['title'])\n",
        "    raw['title'] = raw['title'].str[:512]\n",
        "\n",
        "    scores = []\n",
        "    for i in range(0, len(raw), 32):\n",
        "        scores.extend(sentiment_pipeline(raw['title'].iloc[i:i+32].tolist()))\n",
        "\n",
        "    # Map back\n",
        "    probs = pd.DataFrame(scores)\n",
        "    mapping = {'positive': 'positive', 'LABEL_1': 'positive',\n",
        "               'negative': 'negative', 'LABEL_0': 'negative'}\n",
        "    raw['positive'] = (probs['label'].map(mapping) == 'positive') * probs['score']\n",
        "    raw['negative'] = (probs['label'].map(mapping) == 'negative') * probs['score']\n",
        "    raw['neutral']  = 1 - (raw['positive'] + raw['negative'])\n",
        "\n",
        "    daily = raw.groupby(raw['time'].dt.floor('D'))[['positive','negative','neutral']].mean()\n",
        "    daily['sentiment_score'] = daily['positive'] - (daily['negative'] + 0.5 * daily['neutral'])\n",
        "    return daily[['sentiment_score']]\n",
        "\n",
        "# --- 3.4Â Marketâ€‘sentiment (VIXÂ &Â SPX) ---\n",
        "def calculate_market_sentiment(market: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = market.copy().rename(columns={'vix_price':'vix','spx_price':'spx'})\n",
        "    df['vix_s'] = 1 - scale_series(df[['vix']])\n",
        "    df['spx_s'] = scale_series(df['spx'].pct_change().fillna(0).to_frame())\n",
        "    comp = (df['vix_s'] + df['spx_s']) / 2\n",
        "    df['market_sentiment'] = MinMaxScaler(feature_range=(-1,1)).fit_transform(comp.values.reshape(-1,1))\n",
        "    return df[['market_sentiment']].resample('D').ffill().bfill()\n",
        "\n",
        "# --- 3.5Â Options processing (ATM calls) ---\n",
        "def process_options(path: Path, price_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    opts = pd.read_csv(path, parse_dates=['date', 'exdate'])\n",
        "    opts = opts[opts['cp_flag'] == 'C'].copy()\n",
        "    opts['days_to_expiry'] = (opts['exdate'] - opts['date']).dt.days\n",
        "    opts['mid_price'] = (opts['best_bid'] + opts['best_offer']) / 2\n",
        "    opts['strike_price'] /= 1000\n",
        "\n",
        "    merged = opts.merge(price_df[['spx_price']], left_on='date', right_index=True, how='left')\n",
        "    merged['moneyness'] = merged['spx_price'] / merged['strike_price']\n",
        "\n",
        "    atm = merged[\n",
        "        merged['delta'].abs().between(0.4, 0.6) &\n",
        "        merged['days_to_expiry'].between(10, 30)\n",
        "    ]\n",
        "    features = {\n",
        "        'impl_volatility':'mean', 'delta':'mean', 'gamma':'mean',\n",
        "        'vega':'mean', 'theta':'mean', 'open_interest':'mean',\n",
        "        'volume':'mean', 'mid_price':'mean', 'days_to_expiry':'mean',\n",
        "        'moneyness':'mean'\n",
        "    }\n",
        "    agg = atm.groupby('date').agg(features).rename(columns={'mid_price':'target_option_price'})\n",
        "    return agg.assign(target_option_price=lambda x: x['target_option_price'].shift(-1)).dropna()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91fd2c89",
      "metadata": {
        "id": "91fd2c89"
      },
      "outputs": [],
      "source": [
        "# ---------- 3.6Â Run data pipeline ----------\n",
        "if USE_CACHED == False:\n",
        "    logger.info('Fetching dataâ€¦')\n",
        "\n",
        "    macro_df   = fetch_fast_macro_data(CFG.START_DATE, CFG.END_DATE)\n",
        "\n",
        "    spx = pd.read_csv(CFG.SPX_CSV_PATH, header=3,\n",
        "                      names=['Date','spx_price','High','Low','Open','Volume'],\n",
        "                      parse_dates=['Date']).set_index('Date')\n",
        "    vix = pd.read_csv(CFG.VIX_CSV_PATH, header=3,\n",
        "                      names=['Date','vix_price','High','Low','Open','Volume'],\n",
        "                      parse_dates=['Date']).set_index('Date')\n",
        "\n",
        "    market_df  = spx[['spx_price']].join(vix[['vix_price']], how='outer').loc[CFG.START_DATE:CFG.END_DATE].resample('D').ffill()\n",
        "\n",
        "    news_sent  = fetch_news_sentiment(CFG.FIN_NEWS_PATH, CFG.START_DATE, CFG.END_DATE)\n",
        "    market_sent= calculate_market_sentiment(market_df)\n",
        "    options_df = process_options(CFG.OPTIONS_CSV_PATH, market_df)\n",
        "\n",
        "    # Composite DF\n",
        "    data_base = (macro_df\n",
        "                .join(market_df, how='left')\n",
        "                .join(news_sent, how='left')\n",
        "                .join(market_sent, how='left')\n",
        "                .join(options_df, how='inner')\n",
        "                .ffill().bfill().dropna())\n",
        "\n",
        "    # Greeks PCA\n",
        "    greeks = ['delta','gamma','vega','theta']\n",
        "    pca = PCA(n_components=1)\n",
        "    data_base['greeks_signal_1'] = pca.fit_transform(data_base[greeks])\n",
        "\n",
        "else:\n",
        "    data_base = pd.read_csv(CFG.DRIVE_ROOT / 'processed_data.csv', index_col=0, parse_dates=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Columns to keep (only scaled features + scaled target)\n",
        "inference_cols = (\n",
        "    CFG.MACRO_COLS +\n",
        "    CFG.SENTIMENT_COLS +\n",
        "    CFG.OPTION_COLS +\n",
        "    ['target_option_price']\n",
        ")\n",
        "\n",
        "# Save to inference_eval.csv\n",
        "test_full[inference_cols].to_csv('inference_eval.csv', index=False)\n",
        "print(\"âœ… Saved safe inference_eval.csv\")\n"
      ],
      "metadata": {
        "id": "IvfAEzHLqW8z"
      },
      "id": "IvfAEzHLqW8z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f51fa8f4",
      "metadata": {
        "id": "f51fa8f4"
      },
      "source": [
        "## 4Â Â PyTorchÂ Dataset Construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddc89f35",
      "metadata": {
        "id": "ddc89f35"
      },
      "outputs": [],
      "source": [
        "class UnifiedSequenceDataset(Dataset):\n",
        "    \"\"\"Flexible dataset returning flat, earlyâ€‘fusion or multimodal outputs.\"\"\"\n",
        "    def __init__(self, df: pd.DataFrame, window: int, mode: str,\n",
        "                 feature_groups: List[str]):\n",
        "        self.window = window\n",
        "        self.mode   = mode.lower()\n",
        "        fmap = {'macro': CFG.MACRO_COLS,\n",
        "                'sentiment': CFG.SENTIMENT_COLS,\n",
        "                'options': CFG.OPTION_COLS}\n",
        "        self.cols = sum([fmap[g] for g in feature_groups], [])\n",
        "        self.modal_cols = {k: [c for c in self.cols if c in v] for k, v in fmap.items()}\n",
        "\n",
        "        self.rows = []\n",
        "        for i in range(len(df) - window):\n",
        "            win = df.iloc[i:i+window]\n",
        "            y   = df.iloc[i+window]['target_option_price']\n",
        "            if self.mode == 'flat':\n",
        "                x = win[self.cols].values.flatten()\n",
        "                self.rows.append((x, y))\n",
        "            elif self.mode == 'early_fusion':\n",
        "                x = win[self.cols].values\n",
        "                self.rows.append((x, y))\n",
        "            elif self.mode == 'multimodal':\n",
        "                m = win[self.modal_cols['macro']].values\n",
        "                s = win[self.modal_cols['sentiment']].values\n",
        "                o = win[self.modal_cols['options']].values\n",
        "                self.rows.append((m, s, o, y))\n",
        "            else:\n",
        "                raise ValueError(f'Unsupported mode {self.mode}')\n",
        "\n",
        "    def __len__(self): return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.rows[idx]\n",
        "        if self.mode in ('flat','early_fusion'):\n",
        "            x, y = item\n",
        "            return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "        m, s, o, y = item\n",
        "        return (torch.tensor(m, dtype=torch.float32),\n",
        "                torch.tensor(s, dtype=torch.float32),\n",
        "                torch.tensor(o, dtype=torch.float32),\n",
        "                torch.tensor(y, dtype=torch.float32))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5934959f",
      "metadata": {
        "id": "5934959f"
      },
      "source": [
        "## 5Â Â Model Architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d72ce46",
      "metadata": {
        "id": "2d72ce46"
      },
      "outputs": [],
      "source": [
        "# ---- Helper to build MLP ----\n",
        "def build_mlp(in_dim, hidden, drop):\n",
        "    layers = []\n",
        "    for h, d in zip(hidden, drop):\n",
        "        layers += [nn.Linear(in_dim, h), nn.ReLU()]\n",
        "        if d > 0: layers.append(nn.Dropout(d))\n",
        "        in_dim = h\n",
        "    layers.append(nn.Linear(in_dim, 1))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "# ---- Subâ€‘modules ----\n",
        "class LSTMSubNet(nn.Module):\n",
        "    def __init__(self, in_size, h_size, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(in_size, h_size, num_layers=n_layers,\n",
        "                            batch_first=True, dropout=dropout if n_layers>1 else 0.)\n",
        "        self.fc = nn.Linear(h_size, h_size)\n",
        "    def forward(self, x):\n",
        "        _, (h, _) = self.lstm(x)\n",
        "        return self.fc(h[-1])\n",
        "\n",
        "# ---- 1) Feedâ€‘forward (options only) ----\n",
        "class OptionsOnlyFNN(nn.Module):\n",
        "    def __init__(self, in_dim, hidden, drop):\n",
        "        super().__init__()\n",
        "        self.net = build_mlp(in_dim, hidden, drop)\n",
        "    def forward(self, x): return self.net(x).squeeze(-1)\n",
        "\n",
        "# ---- 2) Feedâ€‘forward (all features) ----\n",
        "class EarlyFusionFNN(nn.Module):\n",
        "    def __init__(self, in_dim, hidden, drop):\n",
        "        super().__init__()\n",
        "        self.net = build_mlp(in_dim, hidden, drop)\n",
        "    def forward(self, x): return self.net(x).squeeze(-1)\n",
        "\n",
        "# ---- 3) LSTM (options only) ----\n",
        "class LSTMOptionsOnly(nn.Module):\n",
        "    def __init__(self, in_size, h, layers, dropout):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(in_size, h, num_layers=layers, batch_first=True,\n",
        "                            dropout=dropout if layers>1 else 0.)\n",
        "        self.fc = nn.Sequential(nn.BatchNorm1d(h), nn.ReLU(), nn.Dropout(dropout),\n",
        "                                nn.Linear(h,1))\n",
        "    def forward(self, x):\n",
        "        _, (h, _) = self.lstm(x)\n",
        "        return self.fc(h[-1]).squeeze(-1)\n",
        "\n",
        "# ---- 4) Bidirectional LSTM (early fusion) ----\n",
        "class LSTMEarlyFusion(nn.Module):\n",
        "    def __init__(self, in_size, h, layers, dropout):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(in_size, h, num_layers=layers, batch_first=True,\n",
        "                            dropout=dropout if layers>1 else 0., bidirectional=True)\n",
        "        self.fc = nn.Sequential(nn.Linear(h*2, h), nn.ReLU(), nn.Linear(h,1))\n",
        "    def forward(self, x):\n",
        "        _, (h, _) = self.lstm(x)\n",
        "        h_cat = torch.cat((h[-2], h[-1]), dim=1)\n",
        "        return self.fc(h_cat).squeeze(-1)\n",
        "\n",
        "# ---- 5) Multimodal LSTM ----\n",
        "class LSTMMultimodal(nn.Module):\n",
        "    def __init__(self, sizes, h, layers, drop):\n",
        "        super().__init__()\n",
        "        self.mac = LSTMSubNet(sizes[0], h, layers, drop)\n",
        "        self.sent= LSTMSubNet(sizes[1], h, layers, drop)\n",
        "        self.opt = LSTMSubNet(sizes[2], h, layers, drop)\n",
        "        self.fuse= nn.Sequential(nn.Linear(h*3,128), nn.ReLU(), nn.Dropout(drop),\n",
        "                                 nn.Linear(128,h), nn.ReLU(), nn.Linear(h,1))\n",
        "    def forward(self, m, s, o):\n",
        "        x = torch.cat([self.mac(m), self.sent(s), self.opt(o)], dim=1)\n",
        "        return self.fuse(x).squeeze(-1)\n",
        "\n",
        "# ---- 6) Attentionâ€‘fusion LSTM ----\n",
        "class AttentionFusionLSTM(nn.Module):\n",
        "    def __init__(self, sizes, h, layers, drop):\n",
        "        super().__init__()\n",
        "        self.mac = LSTMSubNet(sizes[0], h, layers, drop)\n",
        "        self.sent= LSTMSubNet(sizes[1], h, layers, drop)\n",
        "        self.opt = LSTMSubNet(sizes[2], h, layers, drop)\n",
        "\n",
        "        self.pos = nn.Parameter(torch.randn(1, 3, h))\n",
        "        self.attn = nn.MultiheadAttention(h, 1, batch_first=True)\n",
        "        self.norm = nn.LayerNorm(h)\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(h, h//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.Linear(h//2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, m, s, o, return_attention=False):\n",
        "        x = torch.stack([self.mac(m), self.sent(s), self.opt(o)], dim=1) + self.pos\n",
        "        z, attn_weights = self.attn(x, x, x, need_weights=True)\n",
        "        z = self.norm(z + x).mean(1)\n",
        "        out = self.out(z).squeeze(-1)\n",
        "\n",
        "        if return_attention:\n",
        "            return out, attn_weights  # shape: (batch_size, num_heads=1, tokens=3)\n",
        "        return out\n",
        "\n",
        "\n",
        "# ---- 7) Crossâ€‘attention LSTM ----\n",
        "class CrossAttentionLSTM(nn.Module):\n",
        "    def __init__(self, sizes, h, layers, drop):\n",
        "        super().__init__()\n",
        "        self.mac_lstm = nn.LSTM(sizes[0], h, num_layers=layers, batch_first=True, dropout=drop if layers>1 else 0.)\n",
        "        self.sent_lstm = nn.LSTM(sizes[1], h, num_layers=layers, batch_first=True, dropout=drop if layers>1 else 0.)\n",
        "        self.opt_lstm = nn.LSTM(sizes[2], h, num_layers=layers, batch_first=True, dropout=drop if layers>1 else 0.)\n",
        "\n",
        "        # Sent attends to Macro, and Macro attends to Sent\n",
        "        self.sent2mac_attn = nn.MultiheadAttention(h, 1, batch_first=True)\n",
        "        self.mac2sent_attn = nn.MultiheadAttention(h, 1, batch_first=True)\n",
        "\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.LayerNorm(h * 3),  # Concatenate: opt, sent->mac, mac->sent\n",
        "            nn.Linear(h * 3, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, mac, sent, opt):\n",
        "        mac_seq, _ = self.mac_lstm(mac)     # (B, T, H)\n",
        "        sent_seq, _ = self.sent_lstm(sent)  # (B, T, H)\n",
        "        opt_seq, _ = self.opt_lstm(opt)     # (B, T, H)\n",
        "\n",
        "        # Sentiment attends to Macro\n",
        "        sent2mac, _ = self.sent2mac_attn(sent_seq, mac_seq, mac_seq)\n",
        "\n",
        "        # Macro attends to Sentiment\n",
        "        mac2sent, _ = self.mac2sent_attn(mac_seq, sent_seq, sent_seq)\n",
        "\n",
        "        # Feature summary: mean of sequences\n",
        "        sent2mac_feat = sent2mac.mean(1)\n",
        "        mac2sent_feat = mac2sent.mean(1)\n",
        "        opt_feat = opt_seq.mean(1)\n",
        "\n",
        "        feat = torch.cat([opt_feat, sent2mac_feat, mac2sent_feat], dim=1)\n",
        "        return self.fuse(feat).squeeze(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "553d5cbf",
      "metadata": {
        "id": "553d5cbf"
      },
      "source": [
        "## 6Â Â DataÂ Loaders &Â Model Registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa4b0cc3",
      "metadata": {
        "id": "aa4b0cc3"
      },
      "outputs": [],
      "source": [
        "datasets = {\n",
        "    # Feedforward models (flat)\n",
        "    'OptionsOnlyFNN': (\n",
        "        UnifiedSequenceDataset(train_full, CFG.WINDOW, 'flat', ['options']),\n",
        "        UnifiedSequenceDataset(val_full,   CFG.WINDOW, 'flat', ['options']),\n",
        "        UnifiedSequenceDataset(test_full,  CFG.WINDOW, 'flat', ['options'])\n",
        "    ),\n",
        "\n",
        "    'FNN_AllFeatures': (\n",
        "        UnifiedSequenceDataset(train_full, CFG.WINDOW, 'flat', ['macro', 'sentiment', 'options']),\n",
        "        UnifiedSequenceDataset(val_full,   CFG.WINDOW, 'flat', ['macro', 'sentiment', 'options']),\n",
        "        UnifiedSequenceDataset(test_full,  CFG.WINDOW, 'flat', ['macro', 'sentiment', 'options'])\n",
        "    ),\n",
        "\n",
        "    # LSTM models (early fusion)\n",
        "    'LSTM_OptionsOnly': (\n",
        "        UnifiedSequenceDataset(train_full, CFG.WINDOW, 'early_fusion', ['options']),\n",
        "        UnifiedSequenceDataset(val_full,   CFG.WINDOW, 'early_fusion', ['options']),\n",
        "        UnifiedSequenceDataset(test_full,  CFG.WINDOW, 'early_fusion', ['options'])\n",
        "    ),\n",
        "\n",
        "    'LSTM_EarlyFusion': (\n",
        "        UnifiedSequenceDataset(train_full, CFG.WINDOW, 'early_fusion', ['macro', 'sentiment', 'options']),\n",
        "        UnifiedSequenceDataset(val_full,   CFG.WINDOW, 'early_fusion', ['macro', 'sentiment', 'options']),\n",
        "        UnifiedSequenceDataset(test_full,  CFG.WINDOW, 'early_fusion', ['macro', 'sentiment', 'options'])\n",
        "    ),\n",
        "\n",
        "    # LSTM models (multimodal)\n",
        "    'Multimodal_LSTM': (\n",
        "        UnifiedSequenceDataset(train_red, CFG.WINDOW, 'multimodal', ['macro', 'sentiment', 'options']),\n",
        "        UnifiedSequenceDataset(val_red,   CFG.WINDOW, 'multimodal', ['macro', 'sentiment', 'options']),\n",
        "        UnifiedSequenceDataset(test_red,  CFG.WINDOW, 'multimodal', ['macro', 'sentiment', 'options'])\n",
        "    ),\n",
        "\n",
        "    'LSTM_AttnFusion': (\n",
        "        UnifiedSequenceDataset(train_red, CFG.WINDOW, 'multimodal', ['macro', 'sentiment', 'options']),\n",
        "        UnifiedSequenceDataset(val_red,   CFG.WINDOW, 'multimodal', ['macro', 'sentiment', 'options']),\n",
        "        UnifiedSequenceDataset(test_red,  CFG.WINDOW, 'multimodal', ['macro', 'sentiment', 'options'])\n",
        "    ),\n",
        "\n",
        "    'CrossAttention_LSTM': (\n",
        "        UnifiedSequenceDataset(train_red, CFG.WINDOW, 'multimodal', ['macro', 'sentiment', 'options']),\n",
        "        UnifiedSequenceDataset(val_red,   CFG.WINDOW, 'multimodal', ['macro', 'sentiment', 'options']),\n",
        "        UnifiedSequenceDataset(test_red,  CFG.WINDOW, 'multimodal', ['macro', 'sentiment', 'options'])\n",
        "    )\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "# --- 6.4 Build loaders ---\n",
        "loaders = {}\n",
        "for key, (train_ds, val_ds, test_ds) in datasets.items():\n",
        "    bs = 32 if 'Cross' in key or 'Early' in key else 64\n",
        "    loaders[key] = (\n",
        "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
        "        DataLoader(val_ds,   batch_size=bs, shuffle=False),\n",
        "        DataLoader(test_ds,  batch_size=bs, shuffle=False)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49db3dbd",
      "metadata": {
        "id": "49db3dbd"
      },
      "source": [
        "## 7Â Â Training &Â Evaluation Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f6cec88",
      "metadata": {
        "id": "9f6cec88"
      },
      "outputs": [],
      "source": [
        "# --- 7  Training & Evaluation Utilities  (updated) --------------------------\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs:int, lr:float,\n",
        "                patience:int=8):\n",
        "    \"\"\"\n",
        "    Returns dict with epoch-wise train & val losses so that we can inspect\n",
        "    over-fitting afterwards.\n",
        "    \"\"\"\n",
        "    optim     = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                    optim, 'min', factor=.5, patience=10)\n",
        "    loss_fn   = nn.MSELoss()\n",
        "    history   = {'train': [], 'val': []}\n",
        "    best, counter = float('inf'), 0\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        # ---- training step ----\n",
        "        model.train(); running = 0\n",
        "        for batch in train_loader:\n",
        "            optim.zero_grad()\n",
        "            pred, y = forward_pass(model, batch)\n",
        "            loss = loss_fn(pred, y); loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
        "            optim.step()\n",
        "            running += loss.item()\n",
        "        train_loss = running / len(train_loader)\n",
        "\n",
        "        # ---- validation step ----\n",
        "        model.eval(); running = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                pred, y = forward_pass(model, batch)\n",
        "                running += loss_fn(pred, y).item()\n",
        "        val_loss = running / len(val_loader)\n",
        "        history['train'].append(train_loss)\n",
        "        history['val'  ].append(val_loss)\n",
        "\n",
        "        logger.info(f'E{ep:02d}/{epochs} â€” train {train_loss:.5f} | '\n",
        "                    f'val {val_loss:.5f}')\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # ----- early stop -----\n",
        "        if val_loss < best:\n",
        "            best, counter = val_loss, 0\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                logger.info('Early-stopping âœ“'); break\n",
        "\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "    return history\n",
        "\n",
        "\n",
        "def evaluate(model, loader) -> Dict[str,float]:\n",
        "    model.eval()\n",
        "    preds, trues = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            p, y = forward_pass(model, batch)\n",
        "            preds.extend(p.cpu().numpy()); trues.extend(y.cpu().numpy())\n",
        "\n",
        "    preds = np.array(preds).reshape(-1, 1)\n",
        "    trues = np.array(trues).reshape(-1, 1)\n",
        "\n",
        "    # Inverse-transform to original scale\n",
        "    preds = target_scaler.inverse_transform(preds).flatten()\n",
        "    trues = target_scaler.inverse_transform(trues).flatten()\n",
        "\n",
        "    return {\n",
        "        'mae':  mean_absolute_error(trues, preds),\n",
        "        'rmse': np.sqrt(mean_squared_error(trues, preds)),\n",
        "        'r2':   r2_score(trues, preds),\n",
        "        'y': trues, 'y_hat': preds\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31610258",
      "metadata": {
        "id": "31610258"
      },
      "source": [
        "## 8Â Â Experiment Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db595c40",
      "metadata": {
        "id": "db595c40"
      },
      "outputs": [],
      "source": [
        "model_cfg = {\n",
        "    'OptionsOnlyFNN': dict(\n",
        "        cls=OptionsOnlyFNN,\n",
        "        args=dict(in_dim=len(CFG.OPTION_COLS)*CFG.WINDOW,\n",
        "                  hidden=[448, 320], drop=[0.2589736052138709, 0.2932864807416329]),\n",
        "        lr=5.606599300905802e-05, epochs=200  # slightly longer training, lower LR\n",
        "    ),\n",
        "\n",
        "    'FNN_AllFeatures': dict(\n",
        "        cls=EarlyFusionFNN,\n",
        "        args=dict(in_dim=(len(CFG.MACRO_COLS) + len(CFG.SENTIMENT_COLS) + len(CFG.OPTION_COLS)) * CFG.WINDOW,\n",
        "                  hidden=[64,192, 384, 512], drop=[0.21690647686746356, 0.17333303298100486, 0.051245528433709775, 0.28461553354003577]),\n",
        "        lr=3.619917167764799e-05, epochs=200  # reduce overfit tendency\n",
        "    ),\n",
        "\n",
        "    'LSTM_OptionsOnly': dict(\n",
        "        cls=LSTMOptionsOnly,\n",
        "        args=dict(in_size=len(CFG.OPTION_COLS), h=160, layers=1, dropout=0.2548163478893326),\n",
        "        lr=0.0022592222144183815, epochs=100\n",
        "    ),\n",
        "\n",
        "    'LSTM_EarlyFusion': dict(\n",
        "        cls=LSTMEarlyFusion,\n",
        "        args=dict(in_size=len(CFG.MACRO_COLS) + len(CFG.SENTIMENT_COLS) + len(CFG.OPTION_COLS),\n",
        "                  h=224, layers=2, dropout=0.22346200203706412),\n",
        "        lr=0.0003977138694027377, epochs=60\n",
        "    ),\n",
        "\n",
        "    'Multimodal_LSTM': dict(\n",
        "        cls=LSTMMultimodal,\n",
        "        args=dict(sizes=[len(CFG.MACRO_COLS), len(CFG.SENTIMENT_COLS), len(CFG.OPTION_COLS)],\n",
        "                  h=64, layers=1, drop=0.42764880293625906),\n",
        "        lr=0.00012394645403086292, epochs=60  # best-performing model: increase rep capacity\n",
        "    ),\n",
        "\n",
        "    'LSTM_AttnFusion': dict(\n",
        "        cls=AttentionFusionLSTM,\n",
        "        args=dict(sizes=[len(CFG.MACRO_COLS), len(CFG.SENTIMENT_COLS), len(CFG.OPTION_COLS)],\n",
        "                  h=96, layers=1, drop=0.22167129082131226),\n",
        "        lr=0.00031653476644270535, epochs=60\n",
        "    ),\n",
        "\n",
        "    'CrossAttention_LSTM': dict(\n",
        "        cls=CrossAttentionLSTM,\n",
        "        args=dict(sizes=[len(CFG.MACRO_COLS), len(CFG.SENTIMENT_COLS), len(CFG.OPTION_COLS)],\n",
        "                  h=128, layers=3, drop=0.4087067551985453),\n",
        "        lr=0.000357889995198984, epochs=60\n",
        "    )\n",
        "\n",
        "}\n",
        "\n",
        "results       = {}   # metrics for the scoreboard\n",
        "models_cache  = {}   # keep refs for later explainability / saving\n",
        "\n",
        "for name, cfg in model_cfg.items():\n",
        "    logger.info(f'\\nâ–¶ Training {name}')\n",
        "    model = cfg['cls'](**cfg['args']).to(CFG.DEVICE)\n",
        "    train_loader, val_loader, test_loader = loaders[name]\n",
        "\n",
        "    # ---- train & capture loss curves ------------------------------------\n",
        "    history = train_model(model, train_loader, val_loader,\n",
        "                          cfg['epochs'], cfg['lr'])\n",
        "\n",
        "    # ---- quick over-fitting visual --------------------------------------\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    plt.plot(history['train'], label='train')\n",
        "    plt.plot(history['val'],   label='val')\n",
        "    plt.xlabel('epoch'); plt.ylabel('MSE')\n",
        "    plt.title(f'{name} loss curves'); plt.legend(); plt.tight_layout()\n",
        "\n",
        "    if (len(history['val']) > 3 and\n",
        "        history['val'][-1] > min(history['val']) * 1.05):\n",
        "        logger.warning(f'{name}: validation loss is rising â†’ possible over-fit')\n",
        "    plt.savefig(CFG.OUTPUT_DIR / f'{name}_loss_curves.png'); plt.close()\n",
        "\n",
        "    # ---- final evaluation on test set -----------------------------\n",
        "    res = evaluate(model, test_loader)\n",
        "    results[name] = res\n",
        "    models_cache[name] = model.cpu()  # stash for later analysis / saving\n",
        "    save_path = Path(CFG.SAVED_MODEL_DIR) / f\"{name}.pt\"\n",
        "    os.makedirs(CFG.SAVED_MODEL_DIR, exist_ok=True)\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "\n",
        "    # ---- scatter plot: prediction vs actual -----------------------------\n",
        "    fig = plt.figure(figsize=(5, 5))\n",
        "    plt.scatter(res['y'], res['y_hat'], alpha=.3)\n",
        "    lim = max(res['y'].max(), res['y_hat'].max())\n",
        "    plt.plot([0, lim], [0, lim], 'r--')\n",
        "    plt.xlabel('Actual'); plt.ylabel('Predicted')\n",
        "    plt.title(f'{name} â€” Pred vs True'); plt.grid(True); plt.tight_layout()\n",
        "    fig.savefig(CFG.OUTPUT_DIR / f'{name}_pred_vs_actual.png'); plt.close(fig)\n",
        "    logger.info(f'Plots saved â†’ {CFG.OUTPUT_DIR}')\n",
        "\n",
        "# ---- leaderboard print-out -----------------------------------------------\n",
        "print('\\n=== Model Comparison ===')\n",
        "for n, m in results.items():\n",
        "    print(f\"{n:<22} MAE {m['mae']:.4f}  RMSE {m['rmse']:.4f}  RÂ² {m['r2']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c68f65c4",
      "metadata": {
        "id": "c68f65c4"
      },
      "source": [
        "## 9Â Â Hyperâ€‘Parameter Optimisation (Optuna)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_objective(model_name):\n",
        "    def objective(trial):\n",
        "        # Dataset mapping with 3-way split\n",
        "        dataset_map = {\n",
        "            'OptionsOnlyFNN': ('flat', ['options'], train_full, val_full, test_full),\n",
        "            'FNN_AllFeatures': ('flat', ['macro', 'sentiment', 'options'], train_full, val_full, test_full),\n",
        "            'LSTM_OptionsOnly': ('early_fusion', ['options'], train_full, val_full, test_full),\n",
        "            'LSTM_EarlyFusion': ('early_fusion', ['macro', 'sentiment', 'options'], train_full, val_full, test_full),\n",
        "            'Multimodal_LSTM': ('multimodal', ['macro', 'sentiment', 'options'], train_red, val_red, test_red),\n",
        "            'LSTM_AttnFusion': ('multimodal', ['macro', 'sentiment', 'options'], train_red, val_red, test_red),\n",
        "            'CrossAttention_LSTM': ('multimodal', ['macro', 'sentiment', 'options'], train_red, val_red, test_red),\n",
        "        }\n",
        "\n",
        "        mode, groups, train_df_used, val_df_used, test_df_used = dataset_map[model_name]\n",
        "\n",
        "        # Create datasets and dataloaders\n",
        "        train_ds = UnifiedSequenceDataset(train_df_used, CFG.WINDOW, mode, groups)\n",
        "        val_ds   = UnifiedSequenceDataset(val_df_used,   CFG.WINDOW, mode, groups)\n",
        "        test_ds  = UnifiedSequenceDataset(test_df_used,  CFG.WINDOW, mode, groups)\n",
        "\n",
        "        batch_size = trial.suggest_categorical('batch_size', [32, 64])\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "        val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
        "        test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # Model-specific search\n",
        "        if model_name in ['OptionsOnlyFNN', 'FNN_AllFeatures']:\n",
        "            input_dim = len(sum([CFG.MACRO_COLS, CFG.SENTIMENT_COLS, CFG.OPTION_COLS], [])) if model_name == 'FNN_AllFeatures' else len(CFG.OPTION_COLS)\n",
        "            input_dim *= CFG.WINDOW\n",
        "            num_layers = trial.suggest_int('num_layers', 2, 4)\n",
        "            hidden = [trial.suggest_int(f'h{i}', 64, 512, step=64) for i in range(num_layers)]\n",
        "            drop   = [trial.suggest_float(f'd{i}', 0.05, 0.3) for i in range(num_layers)]\n",
        "            lr     = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
        "            model_cls = EarlyFusionFNN if model_name == 'FNN_AllFeatures' else OptionsOnlyFNN\n",
        "            model = model_cls(input_dim, hidden, drop).to(CFG.DEVICE)\n",
        "\n",
        "        elif model_name == 'LSTM_OptionsOnly':\n",
        "            in_size = len(CFG.OPTION_COLS)\n",
        "            h = trial.suggest_int('hidden_size', 64, 256, step=32)\n",
        "            layers = trial.suggest_int('layers', 1, 3)\n",
        "            drop = trial.suggest_float('dropout', 0.1, 0.4)\n",
        "            lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
        "            model = LSTMOptionsOnly(in_size, h, layers, drop).to(CFG.DEVICE)\n",
        "\n",
        "        elif model_name == 'LSTM_EarlyFusion':\n",
        "            in_size = len(CFG.MACRO_COLS) + len(CFG.SENTIMENT_COLS) + len(CFG.OPTION_COLS)\n",
        "            h = trial.suggest_int('hidden_size', 64, 256, step=32)\n",
        "            layers = trial.suggest_int('layers', 1, 3)\n",
        "            drop = trial.suggest_float('dropout', 0.2, 0.5)\n",
        "            lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
        "            model = LSTMEarlyFusion(in_size, h, layers, drop).to(CFG.DEVICE)\n",
        "\n",
        "        elif model_name == 'Multimodal_LSTM':\n",
        "            h = trial.suggest_int('hidden_size', 64, 256, step=32)\n",
        "            layers = trial.suggest_int('layers', 1, 3)\n",
        "            drop = trial.suggest_float('dropout', 0.2, 0.5)\n",
        "            lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
        "            sizes = [len(CFG.MACRO_COLS), len(CFG.SENTIMENT_COLS), len(CFG.OPTION_COLS)]\n",
        "            model = LSTMMultimodal(sizes, h, layers, drop).to(CFG.DEVICE)\n",
        "\n",
        "        elif model_name == 'LSTM_AttnFusion':\n",
        "            h = trial.suggest_int('hidden_size', 64, 256, step=32)\n",
        "            layers = trial.suggest_int('layers', 1, 3)\n",
        "            drop = trial.suggest_float('dropout', 0.2, 0.5)\n",
        "            lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
        "            sizes = [len(CFG.MACRO_COLS), len(CFG.SENTIMENT_COLS), len(CFG.OPTION_COLS)]\n",
        "            model = AttentionFusionLSTM(sizes, h, layers, drop).to(CFG.DEVICE)\n",
        "\n",
        "        elif model_name == 'CrossAttention_LSTM':\n",
        "            h = trial.suggest_int('hidden_size', 64, 256, step=32)\n",
        "            layers = trial.suggest_int('layers', 1, 3)\n",
        "            drop = trial.suggest_float('dropout', 0.2, 0.5)\n",
        "            lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
        "            sizes = [len(CFG.MACRO_COLS), len(CFG.SENTIMENT_COLS), len(CFG.OPTION_COLS)]\n",
        "            model = CrossAttentionLSTM(sizes, h, layers, drop).to(CFG.DEVICE)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "        # Train and evaluate\n",
        "        try:\n",
        "            train_model(model, train_loader, val_loader, epochs=100, lr=lr, patience=10)\n",
        "            result = evaluate(model, test_loader)  # Evaluate on TEST data\n",
        "            return result['rmse']\n",
        "        except Exception as e:\n",
        "            print(f'Trial failed for {model_name}: {e}')\n",
        "            return float('inf')\n",
        "\n",
        "    return objective\n"
      ],
      "metadata": {
        "id": "Bex826kGGEFF"
      },
      "id": "Bex826kGGEFF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = [\n",
        "    'OptionsOnlyFNN',\n",
        "    'FNN_AllFeatures',\n",
        "    'LSTM_OptionsOnly',\n",
        "    'LSTM_EarlyFusion',\n",
        "    'Multimodal_LSTM',\n",
        "    'LSTM_AttnFusion',\n",
        "    'CrossAttention_LSTM'\n",
        "]\n",
        "\n",
        "studies = {}\n",
        "\n",
        "for model_name in model_names:\n",
        "    print(f\"\\nðŸ” Starting Optuna tuning for: {model_name}\")\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(get_objective(model_name), n_trials=10)\n",
        "    studies[model_name] = study\n",
        "    print(f\"\\nâœ… Best RMSE for {model_name}: {study.best_value:.4f}\")\n",
        "    print(\"Best hyperparameters:\")\n",
        "    for k, v in study.best_params.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "id": "Hjkw5kvtGFe9"
      },
      "id": "Hjkw5kvtGFe9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e21836fd",
      "metadata": {
        "id": "e21836fd"
      },
      "source": [
        "## 10Â Â Blackâ€‘Scholes Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "648cda76",
      "metadata": {
        "id": "648cda76"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "# Recover the minimal required options_df from data_base\n",
        "options_df = data_base[[\n",
        "    'impl_volatility', 'moneyness', 'days_to_expiry', 'target_option_price'\n",
        "]].copy()\n",
        "\n",
        "# Reconstruct columns needed for BS model\n",
        "options_df['strike_price'] = data_base['spx_price'] / data_base['moneyness']\n",
        "options_df['spx_price']    = data_base['spx_price']\n",
        "options_df['T']            = data_base['days_to_expiry'] / 365\n",
        "options_df['r']            = data_base['10y_treasury'] / 100\n",
        "\n",
        "# Drop rows with missing values (for safety)\n",
        "options_df = options_df.dropna()\n",
        "\n",
        "def black_scholes_call(S,K,T,r,sigma):\n",
        "    if T<=0 or sigma<=0: return np.maximum(S-K,0)\n",
        "    d1 = (np.log(S/K)+(r+.5*sigma**2)*T)/(sigma*np.sqrt(T))\n",
        "    d2 = d1 - sigma*np.sqrt(T)\n",
        "    return S*norm.cdf(d1) - K*np.exp(-r*T)*norm.cdf(d2)\n",
        "\n",
        "bs = (options_df\n",
        "      .join(macro_df[['10y_treasury']], how='left')\n",
        "      .join(market_df[['spx_price']], how='left')\n",
        "      .assign(strike_price=lambda d: d['spx_price']/d['moneyness'],\n",
        "              T=lambda d: d['days_to_expiry']/365,\n",
        "              r=lambda d: d['10y_treasury']/100)\n",
        "      .dropna())\n",
        "\n",
        "bs['bs_pred_price'] = bs.apply(lambda r: black_scholes_call(r['spx_price'],r['strike_price'],\n",
        "                                                            r['T'],r['r'],r['impl_volatility']), axis=1)\n",
        "actual, pred = bs['target_option_price'].values, bs['bs_pred_price'].values\n",
        "print('Blackâ€‘Scholes â€” MAE %.4f  RMSE %.4f  RÂ² %.4f' %\n",
        "      (mean_absolute_error(actual,pred),\n",
        "       np.sqrt(mean_squared_error(actual,pred)),\n",
        "       r2_score(actual,pred)))\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(actual, pred, alpha=.4)\n",
        "lim=max(actual.max(),pred.max())\n",
        "plt.plot([0,lim],[0,lim],'r--')\n",
        "plt.xlabel('Actual'); plt.ylabel('BS Predicted'); plt.title('Blackâ€‘Scholes vs Actual'); plt.grid(True)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}